# -*- coding: utf-8 -*-
"""Predicting Mortality of Heart Failure .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XJxOjMdub4duFn-IUlZAEPqtdNrg__xa
"""

import pandas as pd

df = pd.read_csv('/content/heart_failure_clinical_records_dataset.csv')
df.head()
df.info()
df.describe()
df.isnull().sum()
df['DEATH_EVENT'].value_counts()
from sklearn.preprocessing import StandardScaler

numerical_features = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']
scaler = StandardScaler()
df[numerical_features] = scaler.fit_transform(df[numerical_features])

# Check for categorical features (excluding target)
# Based on df.info() and the problem description, there are no object type columns
# other than the target if it were an object, which it is not.
# The binary columns are already in a suitable numerical format (0 or 1).
categorical_features = df.select_dtypes(include='object').columns.tolist()
categorical_features = [col for col in categorical_features if col != 'DEATH_EVENT']

if categorical_features:
    print("Categorical features found:", categorical_features)
else:
    print("No additional categorical features found that require encoding.")

display(df.head())
from sklearn.model_selection import train_test_split

X = df.drop('DEATH_EVENT', axis=1)
y = df['DEATH_EVENT']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix

y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred_proba)
conf_matrix = confusion_matrix(y_test, y_pred)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"AUC: {auc:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix)

#Visualize the distributions of numerical features using histograms and box plots.

import matplotlib.pyplot as plt
import seaborn as sns

# Histograms for numerical features
df[numerical_features].hist(bins=15, figsize=(15, 10))
plt.suptitle('Histograms of Numerical Features', y=1.02, ha='center', fontsize='large')
plt.tight_layout()
plt.show()

# Box plots for numerical features
plt.figure(figsize=(15, 10))
sns.boxplot(data=df[numerical_features])
plt.title('Box Plots of Numerical Features')
plt.tight_layout()
plt.show()

#Visualize the relationship between some features and the target variable using count plots and box plots.

# Count plot for binary features vs. DEATH_EVENT
binary_features = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking']
fig, axes = plt.subplots(1, len(binary_features), figsize=(20, 5), sharey=True)
for i, col in enumerate(binary_features):
    sns.countplot(ax=axes[i], x=col, hue='DEATH_EVENT', data=df)
    axes[i].set_title(f'{col} vs. DEATH_EVENT')
plt.tight_layout()
plt.show()

# Box plots for numerical features vs. DEATH_EVENT
fig, axes = plt.subplots(2, 4, figsize=(20, 10))
axes = axes.flatten()
for i, col in enumerate(numerical_features):
    sns.boxplot(ax=axes[i], x='DEATH_EVENT', y=col, data=df)
    axes[i].set_title(f'{col} vs. DEATH_EVENT')
fig.delaxes(axes[len(numerical_features)]) # Remove the empty subplot
plt.tight_layout()
plt.show()

#Identify numerical features, apply scaling, and check for categorical features.
from sklearn.preprocessing import StandardScaler

numerical_features = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']
scaler = StandardScaler()
df[numerical_features] = scaler.fit_transform(df[numerical_features])

# Check for categorical features (excluding target)
# Based on df.info() and the problem description, there are no object type columns
# other than the target if it were an object, which it is not.
# The binary columns are already in a suitable numerical format (0 or 1).
categorical_features = df.select_dtypes(include='object').columns.tolist()
categorical_features = [col for col in categorical_features if col != 'DEATH_EVENT']

if categorical_features:
    print("Categorical features found:", categorical_features)
else:
    print("No additional categorical features found that require encoding.")

display(df.head())

#Split the data into training and testing sets using train_test_split
from sklearn.model_selection import train_test_split

X = df.drop('DEATH_EVENT', axis=1)
y = df['DEATH_EVENT']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

#Calculate and print the evaluation metrics and the confusion matrix for the trained model on the test data.
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix

y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred_proba)
conf_matrix = confusion_matrix(y_test, y_pred)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"AUC: {auc:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix)